{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shri/opt/anaconda3/envs/hackathon/lib/python3.10/site-packages/snowflake/connector/options.py:108: UserWarning: You have an incompatible version of 'pyarrow' installed (12.0.0), please install a version that adheres to: 'pyarrow<10.1.0,>=10.0.1; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from datasets import Dataset, ClassLabel\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(\n",
    "    user=os.environ[\"SF_USER\"],\n",
    "    password=os.environ[\"SF_PWD\"],\n",
    "    account=os.environ[\"SF_ACCOUNT\"],\n",
    "    database=os.environ[\"SF_DB\"],\n",
    "    schema=os.environ[\"SF_SCHEMA\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/kvt_fn2n5kj_cccj1v8w85fc0000gn/T/ipykernel_11140/1906349022.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  alexa_df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "# query the data from Snowflake and create a Pandas dataframe\n",
    "query = 'SELECT INTENT, UTT, PARTITION, SCENARIO FROM ALEXA_MASSIVE_INTENTS_RAW;' #query = 'SELECT * FROM intent_dataset;'\n",
    "alexa_df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INTENT</th>\n",
       "      <th>UTT</th>\n",
       "      <th>PARTITION</th>\n",
       "      <th>SCENARIO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alarm_set</td>\n",
       "      <td>wake me up at five am this week</td>\n",
       "      <td>test</td>\n",
       "      <td>alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alarm_set</td>\n",
       "      <td>wake me up at nine am on friday</td>\n",
       "      <td>train</td>\n",
       "      <td>alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alarm_set</td>\n",
       "      <td>set an alarm for two hours from now</td>\n",
       "      <td>train</td>\n",
       "      <td>alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audio_volume_mute</td>\n",
       "      <td>quiet</td>\n",
       "      <td>test</td>\n",
       "      <td>audio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audio_volume_mute</td>\n",
       "      <td>olly quiet</td>\n",
       "      <td>train</td>\n",
       "      <td>audio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16516</th>\n",
       "      <td>email_query</td>\n",
       "      <td>do i have emails</td>\n",
       "      <td>train</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16517</th>\n",
       "      <td>email_query</td>\n",
       "      <td>what emails are new</td>\n",
       "      <td>train</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16518</th>\n",
       "      <td>email_query</td>\n",
       "      <td>do i have new emails from john</td>\n",
       "      <td>train</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16519</th>\n",
       "      <td>email_query</td>\n",
       "      <td>has john sent me an email</td>\n",
       "      <td>test</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16520</th>\n",
       "      <td>email_query</td>\n",
       "      <td>check email from john</td>\n",
       "      <td>train</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16521 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  INTENT                                  UTT PARTITION   \n",
       "0              alarm_set      wake me up at five am this week      test  \\\n",
       "1              alarm_set      wake me up at nine am on friday     train   \n",
       "2              alarm_set  set an alarm for two hours from now     train   \n",
       "3      audio_volume_mute                                quiet      test   \n",
       "4      audio_volume_mute                           olly quiet     train   \n",
       "...                  ...                                  ...       ...   \n",
       "16516        email_query                     do i have emails     train   \n",
       "16517        email_query                  what emails are new     train   \n",
       "16518        email_query       do i have new emails from john     train   \n",
       "16519        email_query            has john sent me an email      test   \n",
       "16520        email_query                check email from john     train   \n",
       "\n",
       "      SCENARIO  \n",
       "0        alarm  \n",
       "1        alarm  \n",
       "2        alarm  \n",
       "3        audio  \n",
       "4        audio  \n",
       "...        ...  \n",
       "16516    email  \n",
       "16517    email  \n",
       "16518    email  \n",
       "16519    email  \n",
       "16520    email  \n",
       "\n",
       "[16521 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexa_df.drop([\"SCENARIO\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, Trainer, TrainingArguments\n",
    "from datasets import Features, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of unique labels in the 'category' column\n",
    "label_list = alexa_df['INTENT'].unique().tolist()\n",
    "\n",
    "# instantiate a ClassLabel object with the number of classes and the names of the labels\n",
    "num_classes = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Hugging Face dataset\n",
    "features = Features({\"INTENT\": ClassLabel(num_classes=num_classes, names=label_list),\n",
    "                     \"UTT\": Value(\"string\"), \n",
    "                     \"PARTITION\": Value(\"string\"),})\n",
    "dataset = Dataset.from_pandas(alexa_df, features=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_column(\"INTENT\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4c22673c09454a894c31425e7e5c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16521 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24b4d4400e74486832931f1284afd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16521 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbfb54c2509458ebc3df16c40737a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16521 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split dataset into train and test sets based on the value of \"SOURCE\"\n",
    "train_dataset = dataset.filter(lambda example: example[\"PARTITION\"] == \"train\")\n",
    "dev_dataset = dataset.filter(lambda example: example[\"PARTITION\"] == \"dev\")\n",
    "test_dataset = dataset.filter(lambda example: example[\"PARTITION\"] == \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'UTT', 'PARTITION'],\n",
       "    num_rows: 11514\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_samples = 500\n",
    "trunc_train_dataset = train_dataset.shuffle().select(range(max_train_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'UTT', 'PARTITION'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'UTT', 'PARTITION'],\n",
       "    num_rows: 2033\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dev_samples = 90\n",
    "trunc_dev_dataset = dev_dataset.shuffle().select(range(max_dev_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'UTT', 'PARTITION'],\n",
       "    num_rows: 90\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_dev_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'UTT', 'PARTITION'],\n",
       "    num_rows: 2974\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'UTT', 'PARTITION'],\n",
       "    num_rows: 130\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_test_samples = 130\n",
    "trunc_test_dataset = test_dataset.shuffle().select(range(max_test_samples))\n",
    "trunc_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"joaobarroca/distilbert-base-uncased-finetuned-massive-intent-detection-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# load the pre-trained BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87496f9861174f2591aac4fae69d369b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2de49cac28435cb2896a3f66c4037f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe5cf1667334b3c920c7b4de8bc6b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['UTT'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "trunc_train_dataset = trunc_train_dataset.map(tokenize_function, batched=True)\n",
    "trunc_dev_dataset = trunc_dev_dataset.map(tokenize_function, batched=True)\n",
    "trunc_test_dataset = trunc_test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained small BERT model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/kvt_fn2n5kj_cccj1v8w85fc0000gn/T/ipykernel_11140/1996331686.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "import numpy as np\n",
    "def compute_metrics(preds):\n",
    "    logits, labels = preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    evaluation_strategy='epoch',     # evaluate model after every epoch\n",
    "    learning_rate=2e-5,              # learning rate\n",
    "    per_device_train_batch_size=32,  # batch size for training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    num_train_epochs=30,              # number of training epochs\n",
    "    weight_decay=0.01,               # weight decay\n",
    "    push_to_hub=False,               # whether to upload the model checkpoint to the Hub\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=1,\n",
    "    report_to=None #\"wandb\"\n",
    ")\n",
    "\n",
    "# create the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated model to be trained\n",
    "    args=training_args,                  # training arguments\n",
    "    train_dataset=trunc_train_dataset,         # the training dataset\n",
    "    eval_dataset=trunc_dev_dataset,            # the evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shri/opt/anaconda3/envs/hackathon/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshri-adke\u001b[0m (\u001b[33mloyalhealth\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/shri/Desktop/loyal/hackathon23/hackathon2023/wandb/run-20230505_035349-0rshbjvf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/loyalhealth/LoyalHackathon2023/runs/0rshbjvf' target=\"_blank\">rogue-council-2</a></strong> to <a href='https://wandb.ai/loyalhealth/LoyalHackathon2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/loyalhealth/LoyalHackathon2023' target=\"_blank\">https://wandb.ai/loyalhealth/LoyalHackathon2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/loyalhealth/LoyalHackathon2023/runs/0rshbjvf' target=\"_blank\">https://wandb.ai/loyalhealth/LoyalHackathon2023/runs/0rshbjvf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71397152d6a048b7b1112480985965f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.429, 'learning_rate': 1.9714285714285718e-05, 'epoch': 0.14}\n",
      "{'loss': 7.7677, 'learning_rate': 1.942857142857143e-05, 'epoch': 0.29}\n",
      "{'loss': 7.717, 'learning_rate': 1.9142857142857146e-05, 'epoch': 0.43}\n",
      "{'loss': 7.1878, 'learning_rate': 1.885714285714286e-05, 'epoch': 0.57}\n",
      "{'loss': 6.9739, 'learning_rate': 1.8571428571428575e-05, 'epoch': 0.71}\n",
      "{'loss': 7.2568, 'learning_rate': 1.8285714285714288e-05, 'epoch': 0.86}\n",
      "{'loss': 8.2166, 'learning_rate': 1.8e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281ad63e85da43a296953aa639d76ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.313501358032227, 'eval_accuracy': 0.04, 'eval_runtime': 6.075, 'eval_samples_per_second': 4.115, 'eval_steps_per_second': 0.165, 'epoch': 1.0}\n",
      "{'loss': 6.4522, 'learning_rate': 1.7714285714285717e-05, 'epoch': 1.14}\n",
      "{'loss': 4.8362, 'learning_rate': 1.742857142857143e-05, 'epoch': 1.29}\n",
      "{'loss': 6.5282, 'learning_rate': 1.7142857142857142e-05, 'epoch': 1.43}\n",
      "{'loss': 5.8403, 'learning_rate': 1.6857142857142858e-05, 'epoch': 1.57}\n",
      "{'loss': 4.7584, 'learning_rate': 1.6571428571428574e-05, 'epoch': 1.71}\n",
      "{'loss': 5.2669, 'learning_rate': 1.6285714285714287e-05, 'epoch': 1.86}\n",
      "{'loss': 4.6675, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9041705e71548d38baccbd08d8bced0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.936366081237793, 'eval_accuracy': 0.12, 'eval_runtime': 6.5941, 'eval_samples_per_second': 3.791, 'eval_steps_per_second': 0.152, 'epoch': 2.0}\n",
      "{'loss': 5.7732, 'learning_rate': 1.5714285714285715e-05, 'epoch': 2.14}\n",
      "{'loss': 3.4651, 'learning_rate': 1.542857142857143e-05, 'epoch': 2.29}\n",
      "{'loss': 3.339, 'learning_rate': 1.5142857142857144e-05, 'epoch': 2.43}\n",
      "{'loss': 3.8324, 'learning_rate': 1.4857142857142858e-05, 'epoch': 2.57}\n",
      "{'loss': 4.5683, 'learning_rate': 1.4571428571428573e-05, 'epoch': 2.71}\n",
      "{'loss': 3.9617, 'learning_rate': 1.4285714285714287e-05, 'epoch': 2.86}\n",
      "{'loss': 3.1567, 'learning_rate': 1.4e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1292353287204d8691c00d0f7ab7c2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.127197742462158, 'eval_accuracy': 0.12, 'eval_runtime': 6.4295, 'eval_samples_per_second': 3.888, 'eval_steps_per_second': 0.156, 'epoch': 3.0}\n",
      "{'loss': 4.2363, 'learning_rate': 1.3714285714285716e-05, 'epoch': 3.14}\n",
      "{'loss': 3.1839, 'learning_rate': 1.3428571428571429e-05, 'epoch': 3.29}\n",
      "{'loss': 3.4415, 'learning_rate': 1.3142857142857145e-05, 'epoch': 3.43}\n",
      "{'loss': 3.8086, 'learning_rate': 1.2857142857142859e-05, 'epoch': 3.57}\n",
      "{'loss': 3.3967, 'learning_rate': 1.2571428571428572e-05, 'epoch': 3.71}\n",
      "{'loss': 3.3031, 'learning_rate': 1.2285714285714288e-05, 'epoch': 3.86}\n",
      "{'loss': 3.6235, 'learning_rate': 1.2e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2872ad5326c40c4943cba79938a8b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.8603436946868896, 'eval_accuracy': 0.16, 'eval_runtime': 6.4147, 'eval_samples_per_second': 3.897, 'eval_steps_per_second': 0.156, 'epoch': 4.0}\n",
      "{'loss': 3.1943, 'learning_rate': 1.1714285714285716e-05, 'epoch': 4.14}\n",
      "{'loss': 3.5998, 'learning_rate': 1.1428571428571429e-05, 'epoch': 4.29}\n",
      "{'loss': 2.5043, 'learning_rate': 1.1142857142857143e-05, 'epoch': 4.43}\n",
      "{'loss': 3.0056, 'learning_rate': 1.0857142857142858e-05, 'epoch': 4.57}\n",
      "{'loss': 3.181, 'learning_rate': 1.0571428571428572e-05, 'epoch': 4.71}\n",
      "{'loss': 3.8194, 'learning_rate': 1.0285714285714285e-05, 'epoch': 4.86}\n",
      "{'loss': 3.0478, 'learning_rate': 1e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa93070654346ff8b76bf35f60a5f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.7538092136383057, 'eval_accuracy': 0.2, 'eval_runtime': 6.9509, 'eval_samples_per_second': 3.597, 'eval_steps_per_second': 0.144, 'epoch': 5.0}\n",
      "{'loss': 2.8595, 'learning_rate': 9.714285714285715e-06, 'epoch': 5.14}\n",
      "{'loss': 2.4274, 'learning_rate': 9.42857142857143e-06, 'epoch': 5.29}\n",
      "{'loss': 2.6136, 'learning_rate': 9.142857142857144e-06, 'epoch': 5.43}\n",
      "{'loss': 2.5741, 'learning_rate': 8.857142857142858e-06, 'epoch': 5.57}\n",
      "{'loss': 3.3093, 'learning_rate': 8.571428571428571e-06, 'epoch': 5.71}\n",
      "{'loss': 3.2258, 'learning_rate': 8.285714285714287e-06, 'epoch': 5.86}\n",
      "{'loss': 2.9881, 'learning_rate': 8.000000000000001e-06, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2571ea9f7fa34622ad3b18a46a10aae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.7341525554656982, 'eval_accuracy': 0.24, 'eval_runtime': 6.5107, 'eval_samples_per_second': 3.84, 'eval_steps_per_second': 0.154, 'epoch': 6.0}\n",
      "{'loss': 2.3822, 'learning_rate': 7.714285714285716e-06, 'epoch': 6.14}\n",
      "{'loss': 2.6969, 'learning_rate': 7.428571428571429e-06, 'epoch': 6.29}\n",
      "{'loss': 2.4871, 'learning_rate': 7.1428571428571436e-06, 'epoch': 6.43}\n",
      "{'loss': 3.3003, 'learning_rate': 6.857142857142858e-06, 'epoch': 6.57}\n",
      "{'loss': 2.6097, 'learning_rate': 6.571428571428572e-06, 'epoch': 6.71}\n",
      "{'loss': 1.7309, 'learning_rate': 6.285714285714286e-06, 'epoch': 6.86}\n",
      "{'loss': 3.0838, 'learning_rate': 6e-06, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ae055fdbae45dcb3e9b67fd094672f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.7030651569366455, 'eval_accuracy': 0.24, 'eval_runtime': 6.6438, 'eval_samples_per_second': 3.763, 'eval_steps_per_second': 0.151, 'epoch': 7.0}\n",
      "{'loss': 2.3973, 'learning_rate': 5.7142857142857145e-06, 'epoch': 7.14}\n",
      "{'loss': 1.7912, 'learning_rate': 5.428571428571429e-06, 'epoch': 7.29}\n",
      "{'loss': 2.1757, 'learning_rate': 5.142857142857142e-06, 'epoch': 7.43}\n",
      "{'loss': 2.9636, 'learning_rate': 4.857142857142858e-06, 'epoch': 7.57}\n",
      "{'loss': 2.7354, 'learning_rate': 4.571428571428572e-06, 'epoch': 7.71}\n",
      "{'loss': 2.1267, 'learning_rate': 4.2857142857142855e-06, 'epoch': 7.86}\n",
      "{'loss': 2.3166, 'learning_rate': 4.000000000000001e-06, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513cf314f7624560ab5e094d783401d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.6693906784057617, 'eval_accuracy': 0.24, 'eval_runtime': 7.2241, 'eval_samples_per_second': 3.461, 'eval_steps_per_second': 0.138, 'epoch': 8.0}\n",
      "{'loss': 1.8839, 'learning_rate': 3.7142857142857146e-06, 'epoch': 8.14}\n",
      "{'loss': 2.1786, 'learning_rate': 3.428571428571429e-06, 'epoch': 8.29}\n",
      "{'loss': 2.3221, 'learning_rate': 3.142857142857143e-06, 'epoch': 8.43}\n",
      "{'loss': 2.4063, 'learning_rate': 2.8571428571428573e-06, 'epoch': 8.57}\n",
      "{'loss': 2.5236, 'learning_rate': 2.571428571428571e-06, 'epoch': 8.71}\n",
      "{'loss': 2.3093, 'learning_rate': 2.285714285714286e-06, 'epoch': 8.86}\n",
      "{'loss': 2.0998, 'learning_rate': 2.0000000000000003e-06, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b721b93f82964093b96f7b69a1958d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.6437575817108154, 'eval_accuracy': 0.24, 'eval_runtime': 6.7409, 'eval_samples_per_second': 3.709, 'eval_steps_per_second': 0.148, 'epoch': 9.0}\n",
      "{'loss': 1.3398, 'learning_rate': 1.7142857142857145e-06, 'epoch': 9.14}\n",
      "{'loss': 2.5963, 'learning_rate': 1.4285714285714286e-06, 'epoch': 9.29}\n",
      "{'loss': 2.5325, 'learning_rate': 1.142857142857143e-06, 'epoch': 9.43}\n",
      "{'loss': 2.1737, 'learning_rate': 8.571428571428572e-07, 'epoch': 9.57}\n",
      "{'loss': 2.4386, 'learning_rate': 5.714285714285715e-07, 'epoch': 9.71}\n",
      "{'loss': 2.1589, 'learning_rate': 2.8571428571428575e-07, 'epoch': 9.86}\n",
      "{'loss': 2.0316, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cd758fb75d42af99b767fbaa6c9c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.6343860626220703, 'eval_accuracy': 0.24, 'eval_runtime': 6.7288, 'eval_samples_per_second': 3.715, 'eval_steps_per_second': 0.149, 'epoch': 10.0}\n",
      "{'train_runtime': 936.487, 'train_samples_per_second': 1.068, 'train_steps_per_second': 0.075, 'train_loss': 3.6161604336329867, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=70, training_loss=3.6161604336329867, metrics={'train_runtime': 936.487, 'train_samples_per_second': 1.068, 'train_steps_per_second': 0.075, 'train_loss': 3.6161604336329867, 'epoch': 10.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start the training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize metrics on test dataset\n",
    "trainer2 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=trunc_train_dataset,\n",
    "    eval_dataset=trunc_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the model\n",
    "import tarfile\n",
    "save_path = \"./results/\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "with tarfile.open(\"./results/model.tar.gz\", mode='w:gz') as archive:\n",
    "    archive.add(save_path, arcname='.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 3,
  "vscode": {
   "interpreter": {
    "hash": "c20bb9e17ed76fce065404d305c69f4f2a9b3fab08429cf5a46884667d652a33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
